# -*- coding: utf-8 -*-
"""ForensicNET.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CcGRzwRhAKyvnBajgNA3rd2iqQXH33O7
"""

!pip install tensorflow-addons

# Commented out IPython magic to ensure Python compatibility.
import os
from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU
from tensorflow.keras.models import Model
#import necessary libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, utils
import tensorflow_addons as tfa

import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout
from tensorflow.keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import EarlyStopping

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt2
from keras.preprocessing.image import ImageDataGenerator, load_img
from tensorflow.keras.optimizers import RMSprop, Adam, SGD
from keras import regularizers
from keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

"""**Model Architecture**"""

def real_block(input, dim, drop_path=0.0):
     
     #shortcut connection
      shortcurt = input 
      x = layers.Conv2D(filters=dim, kernel_size=7, padding='same', groups=dim)(input)
      x = layers.LayerNormalization(epsilon=1e-6)(x)

      x = layers.Dense(4 * dim)(x)
      x = layers.BatchNormalization()(x)
      x = layers.ReLU()(x)
      x = layers.Dense(dim)(x)
      #Stochastic depth
      drop_depth = tfa.layers.StochasticDepth(drop_path) if drop_path > 0.0 else layers.Activation("linear")

      output = layers.Add()([shortcurt, drop_depth(x)])

      return output

def stem(input, dim):

  x = layers.Conv2D(filters=dim, kernel_size=4, strides=4)(input)
  x = layers.BatchNormalization()(x)

  return x

def downsampling_layers(input, dim):

#   x = layers.LayerNormalization(epsilon=1e-6)(input)
  x = layers.BatchNormalization() (input)
  x = layers.Conv2D(filters=dim, kernel_size=2, strides=2)(x)

  return x

def real_model(input_shape=(224, 224, 3), dims=[96, 192, 384, 768], num_classes=2):

  input = layers.Input(input_shape)

  # stem
  x = stem(input, dims[0])

  # Stage 1 x3, dim[0] = 64
  for _ in range(1):
    x = real_block(x, dims[0])

  # Downsampling layers + stage 2 x3, dim[1] = 128
  x = downsampling_layers(x, dims[1])
  for _ in range(3):
    x = real_block(x, dims[1])

  # Downsampling layers + stage 3 x7, dim[2] = 256
  x = downsampling_layers(x, dims[2])
  for _ in range(6):
    x = real_block(x, dims[2])

  # Downsampling layers + stage 4 x3, dim[3] = 512
  x = downsampling_layers(x, dims[3])
  for _ in range(1):
    x = real_block(x, dims[3])
    
  # Classification head: Global average pool + layer norm + fully connected layer
  x = layers.GlobalAvgPool2D()(x)

  x = layers.LayerNormalization(epsilon=1e-6)(x)    
  x = layers.Dense(units= dims[3], activation='relu') (x)
  x = layers.Dropout(0.2) (x)
  output = layers.Dense(units=num_classes, activation='softmax')(x)

  model = keras.Model(input, output, name='FNet')

  return model

"""**Defining Model Checkpoints**"""

model= real_model()
model.summary()

"""**Training model**"""

epochs = 20
batch_size = 100
init_lr = 1e-5
optimizer = Adam(lr = init_lr, decay = init_lr/epochs)

checkpoint = ModelCheckpoint(filepath='/content/Model_weights/RealNet.h5',
                             save_best_only=True,
                             verbose=1,
                             mode='min',
                             moniter='val_loss'
                            )
reduce_lr = ReduceLROnPlateau(monitor='val_loss', 
                              factor=0.2, 
                              patience=4, 
                              verbose=1
                             )
csv_logger = CSVLogger('training.log')

early_stopping = EarlyStopping(monitor = 'val_acc',
                              min_delta = 0.001,
                              patience =5,
                              verbose = 0,
                              mode = 'auto')

callbacks = [checkpoint, reduce_lr, early_stopping, csv_logger]

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = model.fit(training_set, validation_data = validation_set, epochs = epochs, validation_steps = 100, verbose=1)

plt.figure(figsize=(14,5))
plt.subplot(1,2,2)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['train', 'val'])

plt.subplot(1,2,1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['train', 'val'])
plt.show()

"""**Accuracy On Test Set**"""

_, accu = model.evaluate(test_set)
print('Final Test Acccuracy = {:.3f}'.format(accu*100))

"""**Confusion Matrix and Classification Report**"""

base_path = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/'
image_gen = ImageDataGenerator(rescale=1./255.)

train_flow = image_gen.flow_from_directory(
    base_path + 'train/',
    target_size=(224, 224),
    batch_size=64,
    class_mode='binary'
)

valid_flow = image_gen.flow_from_directory(
    base_path + 'valid/',
    target_size=(224, 224),
    batch_size=64,
    class_mode='binary'
)

test_flow = image_gen.flow_from_directory(
    base_path + 'test/',
    target_size=(224, 224),
    batch_size=64,
    class_mode='binary'
)

import cv2
import numpy as np
from tensorflow.keras import layers
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.callbacks import Callback, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import metrics
import tensorflow as tf
from tqdm import tqdm

y_pred = model.predict(test_flow)
y_test = test_flow.classes

y_test = y_test.reshape(-1, 1)
y_pred = y_pred.reshape(-1, 1)

print("ROC AUC Score:", metrics.roc_auc_score(y_test, y_pred))
print("AP Score:", metrics.average_precision_score(y_test, y_pred))
print()
print(metrics.classification_report(y_test, y_pred > 0.5))



